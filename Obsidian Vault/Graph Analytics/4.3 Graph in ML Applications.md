### Obiettivi
- Il ruolo dei grafi nel Machine Learning workflow.
- Come memorizzare adeguatamente i training data e il modello risultante.
- Algoritmi Graph-based per il machine learning.
- Data analysis con la visualizzazione grafo.
## Dati, Informazione, Conoscenza
Ciascun blocco è un passo verso un livello più alto. Ciascuno step risponde diverse domande sui dati iniziali e ci aggiunge valore. Più arricchiamo i dati con significato e contesto, maggiore conoscenza e intuizioni ne ricaviamo, da poter prendere decisioni migliori, informate e basate sui dati.
![[Pasted image 20250418161459.png|400]]
**Data**:
- I dati sono "raw" (grezzi).
- Semplicemente esistono e **non hanno significato**.
- Esistono in qualunque forma, usabile o no.
**Informazione**:
- L'informazione sono dati a cui viene dato un **significato tramite connessioni relazionali**.
- Questo "significato" può essere utile, ma non deve esserlo obbligatoriamente.
- L'informazione è contenuta nelle descrizioni.
- Le informazioni rispondo alle domande *chi? cosa? quando? dove? quanti?*.
**Conoscenza**:
- La conoscenza è la **collezione adeguata di informazioni**, con l'intento di essere **utile**.
- **Wisdom** è la capacità di formulare giudizi sensati e decisioni.
- Comprendere è un "continuo" che conduce dai dati, attraverso informazioni e conoscenza, e infine al wisdom.
---
## Learning Path
- Dati e informazioni sono riuniti da una o più fonti/sorgenti. Questi dati sono i "training data", su cui si basa l'apprendimento, e vengono gestiti sotto forma di grafo.
- Quando i dati sono organizzati nella forma di Conoscenza e rappresentati da un grafo adatto, algoritmi di ML possono estrarre e costruire intuizioni e Saggezza su di essi.
- I modelli predittivi creati come risultato del training di un algoritmo ML sulla Conoscenza sono memorizzati nuovamente sul grafo, rendendo così la Saggezza permanente e utilizzabile.
- Infine, la visualizzazione mostra i dati in maniera che il cervello possa facilmente comprendere, rendendo la Conoscenza, le intuizioni e la Saggezza derivata accessibile.
![[Pasted image 20250418163556.png]]
---
## Trasformazione dei dati in Grafi
La trasformazione dai dati originali in un grafo può essere fatta mediante diverse tecniche che possono essere classificate in due gruppi:
- **Graph Modeling**: i dati sono convertiti in rappresentazioni a grafo mediante uno schema di modellazione. L'informazione rimane la stessa, solo in un formato differente, o i dati vengono raggruppati per renderli più adatti per essere inseriti nel processo di apprendimento.
- **Graph Construction**: un nuovo grafo viene creato, a partire dai dati disponibili. Il grafo risultante contiene più informazioni di prima.
![[Pasted image 20250418164331.png]]
## Graph Storage e Processo d'Apprendimento
La rappresentazione mediante grafo è utile in quanto:
- **Feature Selection**: un grafo è facilmente interrogabile e può unire (merging) dati da diverse fonti, così trovare ed estrarre la lista delle variabili da usare per il training è reso più facile dall'approccio coi grafi.
- **Data Filtering**: le relazioni facilmente attraversabili rendono semplice il filtraggio di dati inutili prima della fase di training, accelerando il processo di costruzione del modello.
- **Data Preparation**: i grafi rendono semplice la pulizia dei dati, la rimozioni di voci false e l'unione (merge) di dati provenienti da più fonti.
- **Data Enrichment**: estendere i dati con fonti di conoscenza esterne o riutilizzare il risultato della fase di modeling per costruire una base di conoscenza più ampia è semplice con un grafo.
- **Data Formatting**: esportazione dei dati in un qualunque formato: vettori, documenti...

![[Pasted image 20250418164447.png]]
![[Pasted image 20250418165612.png]]

---
# Rappresentazione a Grafo
La trasformazione da dati (strutturati o meno) a grafo può essere eseguita senza perdite, ma questa rappresentazione senza perdite non è sempre necessaria (o desiderabile) ai fini dell'algoritmo di apprendimento.
### Opzione 1 - Progettare un modello a grafo per i dati
Il modello a grafo è una rappresentazione alternativa delle stesse informazioni disponibili nel dataset stesso o in più dataset. I nodi e le relazioni nel grafo sono una mera rappresentazione (aggregata o meno) dei dati disponibili nelle fonti originali. Il grafo funge da sorgente dati connessa che unisce dati provenienti da più fonti eterogenee.
![[Pasted image 20250418170552.png|500]]
### Opzione 2 - Utilizzare metodi di costruzione di grafi
I data items sono memorizzati nel grafo (generalmente come nodi), e un grafo viene creato tramite un meccanismo di costruzione degli edges. Un nuovo grafo è creato a partire dalle informazioni senza relazioni. In questo caso, il grafo risultante incorpora più informazioni rispetto ai set di dati originale. Serve per estrapolare nuove informazioni da un grafo.
![[Pasted image 20250418184935.png|400]]
**Convertire dati nel rispettivo grafo: Steps richiesti**
![[Pasted image 20250418185153.png|350]]

---
## Graph Modeling
1. **Identificare le fonti dati** disponibili per gli algoritmi di training.
2. **Analizzare** i dati disponibili e **valutare** il contenuto, in termini di qualità e quantità.
3. **Progettazione** del graph data model. Secondo i requirements specifici dell'analytics:
	- Identificare le informazioni significative che devono essere estratte dalle fonti dati.
	- Progettare un modello a grafo specifico, considerando i dati disponibili, pattern di accesso e espandibilità.
4. **Definire il data flow** (definito come ETL Process)
5. **Importare dati** nel grafo - inizio del ETL Process (step 3, 4, 5 iterati fino a quando arrivi al modello giusto e giusto processo ETL).
6. Eseguire post-import tasks - preprocessing tasks.
	- Data cleaning, dati incompleti o incorretti.
	- Data enrichment, estendere le fonti dati esistenti con fonti esterne di conoscenza o con conoscenza estratta dai dati stessi.
	- Data merging, i dati vengono da diverse fonti, elementi associati nel data set possono essere uniti in un singolo elemento o possono essere connessi attraverso nuove relazioni.
> **ETL**: è una fase fondamentale per preparare i dati che verranno analizzati come grafo. 
> - **Extract**: si recuperano i dati da una o più fonti.
> - **Transform**: si mappano le entità in nodi, si definiscono le relazioni, e si puliscono i dati.
> - **Load**: i dati vengono caricati in un DB a grafo come Neo4j.

#### Caso d'uso: Torrette Telefoniche
Caso del poliziotto che deve prevedere le mosse future del fuggitivo attraverso le torrette telefoniche.
-> Si può utilizzare i graph models e algoritmi di clustering per rappresentare i movimenti e le posizioni del soggetto. Inoltre possiamo creare un modello predittivo.
Obiettivo: monitorare e creare il modello predittivo capace di identificare i clusters di locazioni importanti per la vita del fuggitivo, così da prevedere e anticipare le sue mosse.
- Opzione 1: i dati dalle torrette possono essere richiesti tramite un permesso ma necessitano di grande pulizia e merging tra le diverse fonti.
- Opzione 2: raccogliere i dati hackerando (non sempre possibile), in questo caso i dati sono già puliti e uniti.
![[Pasted image 20250423101251.png]]
---
# Recommendation Systems
## Collaborative Filtering
L'idea principale del collaborative è quella di utilizzare le informazioni su comportamenti e opinioni passate di altri utenti per predire a quali items l'utente attuale del sistema potrebbe essere interessato.
![[Pasted image 20250423101957.png]]
## User-item Interaction Matrix
L'approccio puramente collaborativo utilizza una matrice di qualunque tipo di interazione user-item (views, acquisti passati, ratings...) come input e produce i seguenti output:
- Una **predizione** (numerica): indica la probabilità con la quale l'utente attuale può apprezzare o no un certo item.
- Una **lista ordinata delle top $n$ raccomandazioni** per un utente basandosi sul valore previsto.
 ![[Pasted image 20250423102407.png]]
 La rilevanza è misurata da una funzione di utilità $f$ che è stimata sui feedback degli users.
## Relevance Function
$f: User * Item ->$ **Relevance Score**
Dove:
- $User$ è un set contenente tutti gli Users e $Item$ è un set di tutti gli Items.
- Questa funzione può essere utilizzata per calcolare la rilevanza per tutti gli elementi in cui nessuna informazione è disponibile.
- Un approccio content-based è possibile se le informazioni sugli utenti (attributi del profilo, preferenze) e items (proprietà) sono presenti.
- Se è disponibile solo un feedback implicito, è richiesto un approccio collaborative filtering.
Dopo aver predetto gli scores di rilevanza per tutti gli items (non comprati dall'utente), possiamo rankarli e mostrare la top $n$ items per l'utente, effettuando dunque recommendation.
##### Esempio
Le celle contengono il ratings degli utenti sugli items, se non è presente allora l'utente non l'ha comprato.
![[Pasted image 20250423103557.png]]
Vogliamo calcolare il possibile interesse dell'item 5 per Bob (non l'ha comprato).
Calcoliamo la prediction utilizzando la similarità tra gli items.
## Similarity tra Items
Per calcolare la similarità tra Items, dobbiamo definire una misura di similarità. La **Cosine Similarity** è la metrica standard negli approcci item-based recommendation: determina la **similarità tra due vettori** calcolando il coseno dell'**angolo** tra loro.
![[Pasted image 20250423104045.png]]
#### Esempio (prec)
Calcolo la similarità tra item 1 e 5.
![[Pasted image 20250423104308.png]]
![[Pasted image 20250423104256.png]]
NB: se abbiamo un DB con 1 milione di items, dobbiamo effettuare 1M x 1M di valori. Possiamo dimezzarli perchè la similarità è commutativa cos(a, b) = cos(b, a) ma rimane costoso.
## Similarità utilizzando Grafi
Utilizziamo un **grafo bipartito** per la rappresentazione dei ratings:
![[Pasted image 20250423105412.png]]
- In un **data-set DB sparso**, la probabilità di ortogonalità (risultato = 0) è abbastanza alto, quindi il **numero di computazioni inutili è alto**.
- Utilizzando i grafi, è semplice trovare tutti gli items che hanno almeno un utente che li ha recensiti entrambi. La similarità può essere eseguita solo sull'item attuale e gli overlapping items (compreso da più cluster), riducendo drasticamente il numero di computazioni.
- Un altro approccio è quello di separare il grafo bipartito in clusters, ed eseguire la similarità solo tra items che appartengono allo stesso cluster.
---
# Algoritmi
Due **approcci**:
- Algoritmo a grafo come **principale algoritmo** di learning.
- Algoritmo a grafo come facilitatore in una più **complessa pipeline di algoritmi**.
## Misure di Centralità
L'Importanza ha diverse definizioni: possono essere utilizzate molte misure di centralità per la rete.
- **PageRank**: conta il numero e la qualità degli edges verso un nodo per arrivare ad una stima approssimativa dell'importanza del nodo.
- **Betweenness**: misura l'importanza del nodo considerando quanto spesso viene attraversato da shortest paths.
#### Caso d'uso: Rischi nella supply chain
Il management dei rischi nella supply chain mira a determinare le suscettibilità della catena alle interruzioni, aka supply chain vulnerability.
Possono essere applicati diversi algoritmi per identificare nodi altamente vulnerabili.

**Misure di Centralità nella supply chain** (caso d'uso):
- **PageRank** permette di identificare nodi che, secondo l'importanza relativa ai nodi a cui sono connessi, hanno alto valore nella rete. In questo caso, interrompere un nodo importante potrebbe influenzare solo una piccola porzione della rete, ma l'interruzione potrebbe essere comunque significante.
- **Betweenness** permette di determinare quali nodi possono avere un'alta influenza nella supply chain dato il flusso della supply chain che li attraversa. I nodi con alta betweenness sono nodi che se rimossi, interrompo la maggior parte del flusso di produzione poiché si trovano su numerosi percorsi.
---
#### Caso d'uso: Trovare keyword in un documento
Vogliamo identificare un set di parole che meglio descrivono un documento. Utilizzando un ranking model basato su grafi permette di trovare la parole più rilevanti nel testo attraverso un metodo di apprendimento.
L'estrazione delle keyword può essere anche utilizzato per costruire un indice automatico per una collezione di documenti, per costruire dizionari di domini specifici, o per eseguire classificazioni del testo.
**Approcci**:
- Il più semplice è quello di utilizzare un **criterio di frequenza relativa** (identificare i termini più frequenti), ma questo metodo manca di sofisticatezza e tipicamente porta risultati scarsi che non si focalizzano sull'importanza delle parole.
- Utilizzando **Supervised Learning Methods**: un sistema viene allenato per riconoscere keywords in un testo, ma sono richiesti numerosi dati per il training del modello in modo accurato (testi con keywords estratte manualmente).
	- I grafi forniscono un meccanismo di estrazione delle keywords utilizzando una rappresentazione a grafo e di un algoritmo di PageRank.
	- **TextRank**: è un modello di ranking a grafo che può essere utilizzato per text processing.
## Text Rank
L'applicazione del TextRank su testi di linguaggi naturali consiste dei seguenti step:
1. **Identifica unità di testo** e li aggiunge al grafo come nodi.
2. **Identifica le relazioni** che connettono le unità di testo. Gli edges possono essere diretti o no, e pesati o no.
3. **Itera l'algoritmo di ranking** (graph based) fino a convergenza o quando viene raggiunto un numero massimo di iterazioni.
4. **Ordina i nodi in base agli scores**, usa gli scores per il ranking ed eventualmente unisce 2 o più unità di testo in una "frase".
#### Co-occurrence Graph
Uno dei metodo più efficienti per l'identificazione di relazioni.
Due nodi sono connessi se entrambi co-occorrono in una finestra di un massimo di N words (tipicamente da 2 a 10). Il peso dell'arco rappresenta quante volte quelle due entità sono comparse insieme.
![[Pasted image 20250423113038.png]]
- Ciascun nodo è inizializzato con un valore = 1, e l'algoritmo esegue fino a convergenza.
- Una volta che sono stati determinati gli scores finali, i nodi vengono ordinati al contrario in base allo score, e post processati.
- Durante il post processing, le parole che appaiono una dopo l'altra nel testo e sono entrambe rilevanti, vengono unite in una singola keyword.
---
#### Caso d'uso: Torrette telefoniche
- Gli algoritmi di clustering permettono di identificare gruppi di torrette: un gruppo di nodi connesso ad un altro tramite un edge fortemente pesato e ad altri nodi con edges più "leggeri" dovrebbe corrispondere ad una locazione dove il soggetto spende molto tempo.
- Il **graph clustering** è un **Unsupervised Learning Method** che mira al raggruppare nodi in clusters, prendendo in considerazione la struttura degli edges tale per cui ci sono molti edges all'interno dei clusters, e pochi nodi tra i clusters stessi.
- Quando il grafo è organizzato in sotto grafi, il prossimo step è utilizzare questa informazione per costruire un **modello predittivo** che è capace di indicare dove il soggetto andrà conoscendo la sua posizione attuale.
- I clusters di torrette possono essere incorporate come **stati di un modello dinamico**.
- Data una sequenza di locazioni visitate dal soggetto, l'algoritmo impara patterns nel suo comportamento ed è capace di calcolare la probabilità con la quale il soggetto si muoverà.
- Dynamic Bayesian network.
---
# Storing e Accessing ML Models
![[Pasted image 20250423114927.png]]
- Questo terzo step nel workflow implica la **consegna di predizioni** agli utenti finali.
- L'output della fase di apprendimento è un modello che contiene i risultati delle inferenze di processo e permette di fare predizioni.
- Il modello deve essere **memorizzato in modo permanente** o in memoria così da poter essere acceduto in un qualunque momento.
- La **velocità** con la quale possiamo accedere al modello influenza la performance di predizione. Questo aspetto è fondamentale per il successo dei progetti ML.

Nel caso precedente dei recommended items e similarità: i risultati processati inizialmente vengono memorizzati così da velocizzare query future.
![[Pasted image 20250423115651.png|300]]
Per calcolare la prediction sul rating dell'item 5 per Bob calcoliamo una somma ponderata:
![[Pasted image 20250423120112.png]]
Nel caso della rappresentazione con **grafo bipartito** per **memorizzarlo** in maniera da interrogarlo facilmente:
- Item-Item Similarity matrix può essere memorizzato in un grafo facilmente.
- Abbiamo **aggiunto un edge** che rappresenta il valore della similarità tra gli items pre-calcolato (0-1).
- Per **ridurre lo storing e le computazioni**:
	- le relazioni tra gli items sono bidirezionali.
	- memorizziamo per ciascun nodo solo le top $k$ relazioni con altri items (invece che N x N relazioni, tanto quelle circa 0 sono inutili).
![[Pasted image 20250423120409.png|400]]