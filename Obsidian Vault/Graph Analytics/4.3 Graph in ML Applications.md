### Obiettivi
- Il ruolo dei grafi nel Machine Learning workflow.
- Come memorizzare adeguatamente i training data e il modello risultante.
- Algoritmi Graph-based per il machine learning.
- Data analysis con la visualizzazione grafo.
## Dati, Informazione, Conoscenza
Ciascun blocco √® un passo verso un livello pi√π alto. Ciascuno step risponde diverse domande sui dati iniziali e ci aggiunge valore. Pi√π arricchiamo i dati con significato e contesto, maggiore conoscenza e intuizioni ne ricaviamo, da poter prendere decisioni migliori, informate e basate sui dati.
![[Pasted image 20250418161459.png|400]]
**Data**:
- I dati sono "raw" (grezzi).
- Semplicemente esistono e **non hanno significato**.
- Esistono in qualunque forma, usabile o no.
**Informazione**:
- L'informazione sono dati a cui viene dato un **significato tramite connessioni relazionali**.
- Questo "significato" pu√≤ essere utile, ma non deve esserlo obbligatoriamente.
- L'informazione √® contenuta nelle descrizioni.
- Le informazioni rispondo alle domande *chi? cosa? quando? dove? quanti?*.
**Conoscenza**:
- La conoscenza √® la **collezione adeguata di informazioni**, con l'intento di essere **utile**.
- **Wisdom** √® la capacit√† di formulare giudizi sensati e decisioni.
- Comprendere √® un "continuo" che conduce dai dati, attraverso informazioni e conoscenza, e infine al wisdom.
---
## Learning Path
- Dati e informazioni sono riuniti da una o pi√π fonti/sorgenti. Questi dati sono i "**training data**", su cui si basa l'apprendimento, e vengono gestiti sotto forma di grafo.
- Quando i dati sono organizzati nella forma di Conoscenza e rappresentati da un grafo adatto, **algoritmi di ML possono estrarre e costruire intuizioni** e Saggezza su di essi.
- I **modelli predittivi** creati come risultato del training di un algoritmo ML sulla Conoscenza **sono memorizzati nuovamente sul grafo**, rendendo cos√¨ la **Saggezza permanente e utilizzabile**.
- Infine, la visualizzazione mostra i dati in maniera che il cervello possa facilmente comprendere, rendendo la Conoscenza, le intuizioni e la Saggezza derivata accessibile.
![[Pasted image 20250418163556.png]]
---
## Trasformazione dei dati in Grafi
> Obiettivo: preparare i dati per il ML, renderli navigabili, puliti e arricchiti.

La trasformazione dai dati originali in un grafo pu√≤ essere fatta mediante diverse tecniche che possono essere classificate in due gruppi:
- **Graph Modeling**: i dati sono convertiti in rappresentazioni a grafo mediante uno schema di modellazione. L'informazione rimane la stessa, solo in un formato differente, o i dati vengono raggruppati per renderli pi√π adatti per essere inseriti nel processo di apprendimento.
- **Graph Construction**: un nuovo grafo viene creato, a partire dai dati disponibili. Il grafo risultante contiene pi√π informazioni di prima.
![[Pasted image 20250418164331.png]]
## Graph Storage e Processo d'Apprendimento
La rappresentazione mediante grafo √® utile in quanto:
- **Feature Selection**: un grafo √® **facilmente interrogabile** e pu√≤ **unire** (merging) **dati da diverse fonti**, cos√¨ trovare ed estrarre la lista delle variabili da usare per il training √® reso pi√π facile dall'approccio coi grafi.
- **Data Filtering**: le relazioni facilmente attraversabili rendono semplice il **filtraggio di dati inutili** prima della fase di training, accelerando il processo di costruzione del modello.
- **Data Preparation**: i grafi rendono **semplice la pulizia dei dati**, la rimozioni di voci false e l'unione (merge) di dati provenienti da pi√π fonti.
- **Data Enrichment**: **estendere i dati** con fonti di conoscenza esterne o riutilizzare il risultato della fase di modeling per costruire una base di conoscenza pi√π ampia √® semplice con un grafo.
- **Data Formatting**: **esportazione dei dati** in un qualunque formato: vettori, documenti...

![[Pasted image 20250418164447.png]]
![[Pasted image 20250418165612.png]]

---
# Rappresentazione a Grafo
La trasformazione da dati (strutturati o meno) a grafo pu√≤ essere eseguita senza perdite, ma questa rappresentazione senza perdite non √® sempre necessaria (o desiderabile) ai fini dell'algoritmo di apprendimento.
### Opzione 1 - Progettare un modello a grafo per i dati
Il modello a grafo √® una rappresentazione alternativa delle stesse informazioni disponibili nel dataset stesso o in pi√π dataset. I nodi e le relazioni nel grafo sono una mera rappresentazione (aggregata o meno) dei dati disponibili nelle fonti originali. **Il grafo funge da sorgente dati connessa che unisce dati provenienti da pi√π fonti eterogenee**.
![[Pasted image 20250418170552.png|500]]
### Opzione 2 - Utilizzare metodi di costruzione di grafi
I data items sono memorizzati nel grafo (generalmente come nodi), e un grafo viene creato tramite un meccanismo di costruzione degli edges. **Un nuovo grafo √® creato a partire dalle informazioni senza relazioni**. In questo caso, **il grafo risultante incorpora pi√π informazioni rispetto ai set di dati originale**. Serve per estrapolare nuove informazioni da un grafo.
![[Pasted image 20250418184935.png|400]]
**Convertire dati nel rispettivo grafo: Steps richiesti**
![[Pasted image 20250418185153.png|350]]

---
## Graph Modeling
1. **Identificare le fonti dati** disponibili per gli algoritmi di training.
2. **Analizzare** i dati disponibili e **valutare** il contenuto, in termini di qualit√† e quantit√†.
3. **Progettazione** del graph data model. Secondo i requirements specifici dell'analytics:
	- Identificare le informazioni significative che devono essere estratte dalle fonti dati.
	- Progettare un modello a grafo specifico, considerando i dati disponibili, pattern di accesso e espandibilit√†.
4. **Definire il data flow** (definito come ETL Process)
5. **Importare dati** nel grafo - inizio del ETL Process (step 3, 4, 5 iterati fino a quando arrivi al modello giusto e giusto processo ETL).
6. **Eseguire post-import tasks** - preprocessing tasks.
	- Data cleaning, dati incompleti o incorretti.
	- Data enrichment, estendere le fonti dati esistenti con fonti esterne di conoscenza o con conoscenza estratta dai dati stessi.
	- Data merging, i dati vengono da diverse fonti, elementi associati nel data set possono essere uniti in un singolo elemento o possono essere connessi attraverso nuove relazioni.
> **ETL**: √® una fase fondamentale per preparare i dati che verranno analizzati come grafo. 
> - **Extract**: si recuperano i dati da una o pi√π fonti.
> - **Transform**: si mappano le entit√† in nodi, si definiscono le relazioni, e si puliscono i dati.
> - **Load**: i dati vengono caricati in un DB a grafo come Neo4j.

#### Caso d'uso: Torrette Telefoniche
Caso del poliziotto che deve prevedere le mosse future del fuggitivo attraverso le torrette telefoniche.
-> Si pu√≤ utilizzare i graph models e algoritmi di clustering per rappresentare i movimenti e le posizioni del soggetto. Inoltre possiamo creare un modello predittivo.
Obiettivo: monitorare e creare il modello predittivo capace di identificare i clusters di locazioni importanti per la vita del fuggitivo, cos√¨ da prevedere e anticipare le sue mosse.
- Opzione 1: i dati dalle torrette possono essere richiesti tramite un permesso ma necessitano di grande pulizia e merging tra le diverse fonti.
- Opzione 2: raccogliere i dati hackerando (non sempre possibile), in questo caso i dati sono gi√† puliti e uniti.
![[Pasted image 20250423101251.png]]
---
# Recommendation Systems
## Collaborative Filtering
L'idea principale del collaborative √® quella di utilizzare le **informazioni su comportamenti e opinioni passate di altri utenti** per predire a quali items l'utente attuale del sistema potrebbe essere interessato.
![[Pasted image 20250423101957.png]]
## User-item Interaction Matrix
L'approccio puramente collaborativo utilizza una matrice di qualunque tipo di interazione user-item (views, acquisti passati, ratings...) come input e produce i seguenti output:
- Una **predizione** (numerica): indica la probabilit√† con la quale l'utente attuale pu√≤ apprezzare o no un certo item.
- Una **lista ordinata delle top $n$ raccomandazioni** per un utente basandosi sul valore previsto.
 ![[Pasted image 20250423102407.png]]
 La rilevanza √® misurata da una funzione di utilit√† $f$ che √® stimata sui feedback degli users.
## Relevance Function
$f: User * Item ->$ **Relevance Score**
Dove:
- $User$ √® un set contenente tutti gli Users e $Item$ √® un set di tutti gli Items.
- Questa funzione pu√≤ essere utilizzata per **calcolare la rilevanza per tutti gli elementi** in cui nessuna informazione √® disponibile.
- Un approccio content-based **√® possibile se le informazioni** sugli utenti (attributi del profilo, preferenze) e items (propriet√†) **sono presenti**.
- Se √® disponibile solo un feedback implicito, √® richiesto un approccio collaborative filtering.
Dopo aver predetto gli scores di rilevanza per tutti gli items (non comprati dall'utente), possiamo rankarli e mostrare la top $n$ items per l'utente, effettuando dunque recommendation.
##### Esempio
Le celle contengono il ratings degli utenti sugli items, se non √® presente allora l'utente non l'ha comprato.
![[Pasted image 20250423103557.png]]
Vogliamo calcolare il possibile interesse dell'item 5 per Bob (non l'ha comprato).
Calcoliamo la prediction utilizzando la similarit√† tra gli items.
## Similarity tra Items
Per calcolare la similarit√† tra Items, dobbiamo definire una misura di similarit√†. La **Cosine Similarity** √® la metrica standard negli approcci item-based recommendation: determina la **similarit√† tra due vettori** calcolando il coseno dell'**angolo** tra loro.
![[Pasted image 20250423104045.png]]
#### Esempio (prec)
Calcolo la similarit√† tra item 1 e 5.
![[Pasted image 20250423104308.png]]
![[Pasted image 20250423104256.png]]
NB: se abbiamo un DB con 1 milione di items, dobbiamo effettuare 1M x 1M di valori. Possiamo dimezzarli perch√® la similarit√† √® commutativa cos(a, b) = cos(b, a) ma rimane costoso.
## Similarit√† utilizzando Grafi
Utilizziamo un **grafo bipartito** per la rappresentazione dei ratings:
![[Pasted image 20250423105412.png]]
- In un **data-set DB sparso**, la probabilit√† di ortogonalit√† (risultato = 0) √® abbastanza alto, quindi il **numero di computazioni inutili √® alto**.
- Utilizzando i grafi, √® semplice trovare tutti gli items che hanno almeno un utente che li ha recensiti entrambi. La similarit√† pu√≤ essere eseguita solo sull'item attuale e gli overlapping items (compreso da pi√π cluster), riducendo drasticamente il numero di computazioni.
- Un altro approccio √® quello di separare il grafo bipartito in clusters, ed eseguire la similarit√† solo tra items che appartengono allo stesso cluster.
---
# Algoritmi
Due **approcci**:
- Algoritmo a grafo come **principale algoritmo** di learning.
- Algoritmo a grafo come facilitatore in una pi√π **complessa pipeline di algoritmi**.
## Misure di Centralit√†
L'Importanza ha diverse definizioni: possono essere utilizzate molte misure di centralit√† per la rete.
- **PageRank**: conta il numero e la qualit√† degli edges verso un nodo per arrivare ad una stima approssimativa dell'importanza del nodo.
- **Betweenness**: misura l'importanza del nodo considerando quanto spesso viene attraversato da shortest paths.
#### Caso d'uso: Rischi nella supply chain
Il management dei rischi nella supply chain mira a determinare le suscettibilit√† della catena alle interruzioni, aka supply chain vulnerability.
Possono essere applicati diversi algoritmi per identificare nodi altamente vulnerabili.

**Misure di Centralit√† nella supply chain** (caso d'uso):
- **PageRank** permette di identificare nodi che, secondo l'importanza relativa ai nodi a cui sono connessi, hanno alto valore nella rete. In questo caso, interrompere un nodo importante potrebbe influenzare solo una piccola porzione della rete, ma l'interruzione potrebbe essere comunque significante.
- **Betweenness** permette di determinare quali nodi possono avere un'alta influenza nella supply chain dato il flusso della supply chain che li attraversa. I nodi con alta betweenness sono nodi che se rimossi, interrompo la maggior parte del flusso di produzione poich√© si trovano su numerosi percorsi.
---
#### Caso d'uso: Trovare keyword in un documento
Vogliamo identificare un set di parole che meglio descrivono un documento. Utilizzando un ranking model basato su grafi permette di trovare la parole pi√π rilevanti nel testo attraverso un metodo di apprendimento.
L'estrazione delle keyword pu√≤ essere anche utilizzato per costruire un indice automatico per una collezione di documenti, per costruire dizionari di domini specifici, o per eseguire classificazioni del testo.
**Approcci**:
- Il pi√π semplice √® quello di utilizzare un **criterio di frequenza relativa** (identificare i termini pi√π frequenti), ma questo metodo manca di sofisticatezza e tipicamente porta risultati scarsi che non si focalizzano sull'importanza delle parole.
- Utilizzando **Supervised Learning Methods**: un sistema viene allenato per riconoscere keywords in un testo, ma sono richiesti numerosi dati per il training del modello in modo accurato (testi con keywords estratte manualmente).
	- I grafi forniscono un meccanismo di estrazione delle keywords utilizzando una rappresentazione a grafo e di un algoritmo di PageRank.
	- **TextRank**: √® un modello di ranking a grafo che pu√≤ essere utilizzato per text processing.
## Text Rank
L'applicazione del TextRank su testi di linguaggi naturali consiste dei seguenti step:
1. **Identifica unit√† di testo** e li aggiunge al grafo come nodi.
2. **Identifica le relazioni** che connettono le unit√† di testo. Gli edges possono essere diretti o no, e pesati o no.
3. **Itera l'algoritmo di ranking** (graph based) fino a convergenza o quando viene raggiunto un numero massimo di iterazioni.
4. **Ordina i nodi in base agli scores**, usa gli scores per il ranking ed eventualmente unisce 2 o pi√π unit√† di testo in una "frase".
#### Co-occurrence Graph
Uno dei metodo pi√π efficienti per l'identificazione di relazioni.
Due nodi sono connessi se entrambi co-occorrono in una finestra di un massimo di N words (tipicamente da 2 a 10). Il peso dell'arco rappresenta quante volte quelle due entit√† sono comparse insieme.
![[Pasted image 20250423113038.png]]
- Ciascun nodo √® inizializzato con un valore = 1, e l'algoritmo esegue fino a convergenza.
- Una volta che sono stati determinati gli scores finali, i nodi vengono ordinati al contrario in base allo score, e post processati.
- Durante il post processing, le parole che appaiono una dopo l'altra nel testo e sono entrambe rilevanti, vengono unite in una singola keyword.
---
#### Caso d'uso: Torrette telefoniche
- Gli algoritmi di clustering permettono di identificare gruppi di torrette: un gruppo di nodi connesso ad un altro tramite un edge fortemente pesato e ad altri nodi con edges pi√π "leggeri" dovrebbe corrispondere ad una locazione dove il soggetto spende molto tempo.
- Il **graph clustering** √® un **Unsupervised Learning Method** che mira al raggruppare nodi in clusters, prendendo in considerazione la struttura degli edges tale per cui ci sono molti edges all'interno dei clusters, e pochi nodi tra i clusters stessi.
- Quando il grafo √® organizzato in sotto grafi, il prossimo step √® utilizzare questa informazione per costruire un **modello predittivo** che √® capace di indicare dove il soggetto andr√† conoscendo la sua posizione attuale.
- I clusters di torrette possono essere incorporate come **stati di un modello dinamico**.
- Data una sequenza di locazioni visitate dal soggetto, l'algoritmo impara patterns nel suo comportamento ed √® capace di calcolare la probabilit√† con la quale il soggetto si muover√†.
- Dynamic Bayesian network.
---
# Storing e Accessing ML Models
![[Pasted image 20250423114927.png]]
- Questo terzo step nel workflow implica la **consegna di predizioni** agli utenti finali.
- L'output della fase di apprendimento √® un modello che contiene i risultati delle inferenze di processo e permette di fare predizioni.
- Il modello deve essere **memorizzato in modo permanente** o in memoria cos√¨ da poter essere acceduto in un qualunque momento.
- La **velocit√†** con la quale possiamo accedere al modello influenza la performance di predizione. Questo aspetto √® fondamentale per il successo dei progetti ML.

Nel caso precedente dei recommended items e similarit√†: i risultati processati inizialmente vengono memorizzati cos√¨ da velocizzare query future.
![[Pasted image 20250423115651.png|300]]
Per calcolare la prediction sul rating dell'item 5 per Bob calcoliamo una somma ponderata:
![[Pasted image 20250423120112.png]]
Nel caso della rappresentazione con **grafo bipartito** per **memorizzarlo** in maniera da interrogarlo facilmente:
- Item-Item Similarity matrix pu√≤ essere memorizzato in un grafo facilmente.
- Abbiamo **aggiunto un edge** che rappresenta il valore della similarit√† tra gli items pre-calcolato (0-1).
- Per **ridurre lo storing e le computazioni**:
	- le relazioni tra gli items sono bidirezionali.
	- memorizziamo per ciascun nodo solo le top $k$ relazioni con altri items (invece che N x N relazioni, tanto quelle circa 0 sono inutili).
![[Pasted image 20250423120409.png|400]]

---
---
---
---
---
## üéØ **Obiettivi principali**

- **Grafi nel ML**: usati per rappresentare, analizzare, visualizzare dati e modelli.
    
- **Storage ML**: memorizzare in modo efficiente dati e modelli.
    
- **Algoritmi a grafo**: per learning, raccomandazione, centralit√†, clustering.
    
- **Visualizzazione**: trasforma dati in insight facilmente interpretabili.
    

---

## üß± **Dati ‚Üí Informazione ‚Üí Conoscenza**

- **Dati (Raw)**: non hanno significato da soli.
    
- **Informazione**: significato dato dalle **connessioni tra dati**.
    
- **Conoscenza**: informazione strutturata con **intento utile** ‚Üí base per il **decision making**.
    
- **Saggezza (Wisdom)**: capacit√† di agire in modo corretto basandosi sulla conoscenza.
    

---

## üß© **Learning Path**

- I **dati grezzi** diventano **grafi strutturati**.
    
- Il ML estrae **pattern** ‚Üí genera **conoscenza**.
    
- I modelli predittivi sono memorizzati nel grafo.
    
- La **visualizzazione** rende i risultati **accessibili e comprensibili**.
    

---

## üõ†Ô∏è **Trasformazione in Grafi**

- **Graph Modeling**: stessa informazione ‚Üí nuovo formato (grafo).
    
- **Graph Construction**: nuova informazione derivata ‚Üí grafo arricchito.
    
- Obiettivo: **preparare i dati per il ML**, renderli navigabili, puliti e arricchiti.
    

---

## üß∞ **Graph Storage e ML Process**

- **Feature Selection**: trovare variabili rilevanti √® pi√π semplice.
    
- **Data Filtering**: si eliminano dati non utili.
    
- **Data Preparation**: unione, pulizia, standardizzazione.
    
- **Data Enrichment**: integrazione con fonti esterne.
    
- **Data Formatting**: esportazione in formati per il ML (es. vettori).
    

---

## üßÆ **Modellare un Grafo**

1. Identificare **fonti dati**
    
2. Valutare **qualit√† e quantit√†**
    
3. Progettare **modello a grafo**
    
4. Definire **ETL (Extract, Transform, Load)**
    
5. **Importare e trasformare** i dati
    
6. **Post-process**: pulizia, merging, enrichment
    

---

## üëÆ **Caso d‚Äôuso: Torrette Telefoniche**

- Raccogliere dati sui movimenti di un soggetto.
    
- Creare **clusters** (luoghi significativi).
    
- Usare modelli predittivi per anticipare movimenti.
    
- Usare dati autorizzati o ‚Äúacquisiti‚Äù da fonti esterne.
    

---

## üí° **Recommendation Systems**

- **Collaborative Filtering**: predizioni basate su comportamenti simili di altri utenti.
    
- **User-Item Matrix**: rappresenta interazioni (views, acquisti, rating).
    
- **Relevance Function**: calcola un **relevance score** per ogni item.
    
- **Cosine Similarity**: metrica standard per valutare similarit√† tra items.
    

---

## üîó **Grafi per la Similarit√†**

- **Grafo bipartito**: utenti ‚Üî items
    
- **Clusterizzazione**: riduce la quantit√† di confronti da eseguire.
    
- **Precalcolo** delle similarit√† ‚Üí miglioramento performance.
    

---

## üìä **Algoritmi a Grafo**

- **Usati direttamente** per ML (clustering, ranking)
    
- **Usati come supporto** in pipeline pi√π ampie.
    

### üîù Misure di Centralit√†

- **PageRank**: stima l‚Äôimportanza di un nodo secondo il numero/qualit√† dei link entranti.
    
- **Betweenness**: misura quanto un nodo √® attraversato da cammini minimi.
    

---

## üè≠ **Supply Chain & Centralit√†**

- **PageRank** ‚Üí trova nodi centrali.
    
- **Betweenness** ‚Üí identifica **colli di bottiglia** critici per il flusso produttivo.
    

---

## üìë **Keyword Extraction & TextRank**

- **TextRank**: algoritmo di **ranking a grafo** applicato ai testi.
    
- **Co-occurrence Graph**: due parole collegate se appaiono vicine nel testo.
    
- Step:
    
    1. Identificare parole chiave
        
    2. Creare relazioni
        
    3. Calcolare ranking
        
    4. Selezionare top keyword
        

---

## üì° **Torrette & Clustering**

- Clustering per raggruppare celle visitate frequentemente.
    
- **Modello dinamico**: usa i cluster come stati per **prevedere** spostamenti futuri.
    
- Approccio probabilistico (es. **Dynamic Bayesian Network**).
    

---

## üíæ **Memorizzare Modelli ML**

- I modelli devono essere **persistenti** e **accessibili velocemente**.
    
- Es: matrice di similarit√† item-item memorizzata come **grafo con pesi** sugli archi.
    
- Per efficienza:
    
    - Relazioni bidirezionali
        
    - Solo top-K relazioni conservate
        

---

## üîö Conclusione: Ruolo dei Grafi nel ML

- **Organizzazione strutturata** e relazionale dei dati.
    
- **Velocit√†** in feature extraction e modellazione.
    
- **Efficienza** in memorizzazione e accesso ai risultati.
    
- **Chiarezza** e comunicazione tramite visualizzazione.